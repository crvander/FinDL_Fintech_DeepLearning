# 7. References

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural in-
formation pro- cessing systems, pages 5998–6008.

Bruno Taborda, Ana de Almeida, Jose Carlos Dias, Fernando Batista, Ricardo Ribeiro. (2021).  ́
”Stock Market Tweets Data.” Web.

Deli Chen, Shuming Ma, Keiko Harimoto, Ruihan Bao, Qi Su, and Xu Sun. 2019d. Group, extract and aggregate: Summarizing a large amount of finance news for forex movement prediction. In Proceedings of the Second Workshop on Economics and Natural Language Processing, pages 41–50, Hong Kong. Association for Computational Linguistics.

Dussa, A. (2020). Finetuning Pre-Trained Language Models for Sentiment Classification of
COVID19 Tweets. ARROW@TU Dublin. https://arrow.tudublin.ie/scschcomdis/224/

Huang, Allen and Wang, Hui and Yang, Yi, FinBERT - A Large Language Model for Extracting Information from Financial Text (July 28, 2020). Contemporary Accounting Research, Forthcoming,

Available at SSRN: https://ssrn.com/abstract=3910214 or http://dx.doi.org/10.2139/ssrn.3910214 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics

Jintao Liu, Hongfei Lin, Xikai Liu, Bo Xu, Yuqi Ren, Yufeng Diao, and Liang Yang. 2019.
Transformer- based capsule network for stock movement prediction. In Proceedings of the First
Workshop on Financial Technology and Natural Language Processing, pages 66–73, Macao, China

JOSEPH Engelberg and Pengjie Gao. 2011. In search of attention. The Journal of Finance,
66(5):1461– 1499.

Khalid, U., Beg, M., Arshad, M. (2021). RUBERT: A Bilingual Roman Urdu BERT Using Cross
Lingual Transfer Learning. Retrieved 5 December 2022, from https://arxiv.org/abs/2102.11278

Linyi Yang, Ruihai Dong, Tin Lok James Ng, and Yang Xu. 2019. Leveraging BERT to improve the FEARS index for stock forecasting. In Proceedings of the First Workshop on Financial Technology and Natural Language Processing, pages 54–60, Macao, China.

Malo, Pekka Sinha, Ankur Takala, Pyry Korhonen, Pekka Wallenius, Jyrki. (2013).
FinancialPhraseBank-v1.0.

Radford, Alec, et al. ”Improving language understanding by generative pre-training.” (2018).

Sara Sabour, Nicholas Frosst, and Geoffrey E Hin- ton. 2017. Dynamic routing between capsules.
In Advances in neural information processing systems, pages 3856–3866.

Shubham Jangir. 2021. Finetuning BERT and XLNet for Sentiment Analysis of Stock Market
Tweets using Mixout and Dropout Regularization, Dublin, Technological University of Dublin

“Spacy 101: Everything You Need to Know · Spacy Usage Documentation.” SpaCy 101: Everything You Need to Know, https://spacy.io/usage/spacy-101.

Stanislau Semeniuta, Aliaksei Severyn, and Erhardt Barth. 2017. A hybrid convolutional variational autoencoder for text generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 627–637, Copenhagen, Denmark. Association for Computational Linguistics.

Yash Chaudhary (2020). Stock-Market Sentiment Dataset: Positive-Negative sentiment at stock
tweets

Yumo Xu and Shay B. Cohen. 2018. Stock movement prediction from tweets and historical prices.

In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1970–1979, Melbourne, Australia. Association for Computational
Linguistics.

Ziniu Hu, Weiqing Liu, Jiang Bian, Xuanzhe Liu, and Tie-Yan Liu. 2018. Listening to chaotic
whispers: A deep learning framework for news-oriented stock trend prediction. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 261–269. ACM

