# Abstract

Textual data in the financial domain is becoming increasingly important as the number of financial documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information has gained popularity among researchers, deep learning has boosted the development of effective financial text mining models and made significant breakthroughs in various Natural Language Processing tasks. State-of-the-art models such as BERT (Devlin et al., 2019) model developed by Google, GPT2 (Radford et al., 2018) by OpenAI, XL- Net (Yang et al.,2019), pre-trained on a large scale of unlabeled texts from various corpuses, have shown their effectiveness by achieving good results on general domain data. 

However, these models are not effective enough on finance specific language and semantics, limiting the accuracy that financial users can expect from their NLP models. In this project, we will finetune the popular transformers based on the pretrained models provided by Hugging Face using our collected manually labeled financial data. Furthermore, we provide web scraping utilities along with the specific financial domain language models, in order to better assist users in financial domain capture market trends and be alerted by the risk factors.
